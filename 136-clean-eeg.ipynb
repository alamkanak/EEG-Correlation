{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import mne\n",
    "from mne.io import read_raw_eeglab, read_epochs_eeglab\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy import fftpack\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import math\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from livelossplot.tf_keras import PlotLossesCallback\n",
    "\n",
    "import autosklearn.regression\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import multiprocessing\n",
    "from tpot import TPOTRegressor\n",
    "from oct2py import octave\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from joblib import wrap_non_picklable_objects\n",
    "import json\n",
    "import pickle\n",
    "import os.path\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import timeit\n",
    "from skimage.transform import resize\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, callbacks\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.genmod import bayes_mixed_glm as glm\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from spectrum import arburg, arma2psd, pburg\n",
    "import pylab\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.integrate import simps\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeglab_path = '/home/raquib/Documents/MATLAB/eeglab2019_0/functions/'\n",
    "octave.addpath(eeglab_path + 'guifunc')\n",
    "octave.addpath(eeglab_path + 'popfunc')\n",
    "octave.addpath(eeglab_path + 'adminfunc')\n",
    "octave.addpath(eeglab_path + 'sigprocfunc')\n",
    "octave.addpath(eeglab_path + 'miscfunc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eegs = ['data/original/sub03/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub03/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub03/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub03/exp03/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub03/exp03/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub03/exp03/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub04/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub04/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub04/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    "#  'data/original/sub05/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',  # NO MEP\n",
    "#  'data/original/sub05/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',  # NO MEP\n",
    " 'data/original/sub05/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub06/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub06/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub06/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub06/exp02/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub07/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub07/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub07/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub08/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub08/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub08/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub08/exp02/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub08/exp03/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub08/exp03/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub08/exp03/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub09/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub10/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub10/exp02/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub10/exp02/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub10/exp02/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub11/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub12/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub12/exp02/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub12/exp03/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub12/exp03/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub12/exp03/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    "#  'data/original/sub13/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat', # Left handed\n",
    "# 'data/original/sub14/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',   # BAD MEP\n",
    "#  'data/original/sub14/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat', # BAD MEP\n",
    "# 'data/original/sub14/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',   # BAD MEP\n",
    " 'data/original/sub15/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub15/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub15/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub16/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub16/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub16/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub17/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub18/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub18/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub18/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub19/exp01/eeg/SP 110RMT r1/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub19/exp01/eeg/SP 110RMT r2/06-clean-prestimulus-hjorth.mat',\n",
    " 'data/original/sub19/exp01/eeg/SP 110RMT r3/06-clean-prestimulus-hjorth.mat']\n",
    "\n",
    "\n",
    "rejected_trials = [\n",
    "    {'sub': 'sub03', 'exp': 'exp02', 'run': 'r1', 'trial': '*'},\n",
    "    {'sub': 'sub03', 'exp': 'exp02', 'run': 'r2', 'trial': '*'},\n",
    "    {'sub': 'sub05', 'exp': 'exp01', 'run': 'r3', 'trial': '*'},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r1', 'trial': '*'},\n",
    "    {'sub': 'sub09', 'exp': 'exp01', 'run': 'r1', 'trial': '*'},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r1', 'trial': '*'},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r2', 'trial': '*'},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': '*'},\n",
    "    {'sub': 'sub11', 'exp': 'exp01', 'run': 'r1', 'trial': '*'},\n",
    "    {'sub': 'sub11', 'exp': 'exp02', 'run': 'r1', 'trial': '*'},\n",
    "    {'sub': 'sub12', 'exp': 'exp01', 'run': 'r1', 'trial': '*'},\n",
    "    {'sub': 'sub15', 'exp': 'exp01', 'run': 'r1', 'trial': '*'},\n",
    "    {'sub': 'sub08', 'exp': 'exp03', 'run': 'r1', 'trial': 0},\n",
    "    {'sub': 'sub09', 'exp': 'exp01', 'run': 'r1', 'trial': 0},\n",
    "    {'sub': 'sub11', 'exp': 'exp02', 'run': 'r1', 'trial': 14},\n",
    "    {'sub': 'sub11', 'exp': 'exp02', 'run': 'r1', 'trial': 38},\n",
    "    {'sub': 'sub12', 'exp': 'exp01', 'run': 'r1', 'trial': 0},\n",
    "    {'sub': 'sub12', 'exp': 'exp02', 'run': 'r1', 'trial': 14},\n",
    "    {'sub': 'sub12', 'exp': 'exp02', 'run': 'r1', 'trial': 36},\n",
    "    {'sub': 'sub12', 'exp': 'exp02', 'run': 'r1', 'trial': 44},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 11},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 15},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 16},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 17},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 24},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 25},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 26},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 27},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 28},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 29},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 30},\n",
    "    {'sub': 'sub12', 'exp': 'exp03', 'run': 'r1', 'trial': 32},\n",
    "    {'sub': 'sub03', 'exp': 'exp01', 'run': 'r2', 'trial': 0},\n",
    "    {'sub': 'sub03', 'exp': 'exp01', 'run': 'r3', 'trial': 0},\n",
    "    {'sub': 'sub03', 'exp': 'exp01', 'run': 'r3', 'trial': 5},\n",
    "    {'sub': 'sub03', 'exp': 'exp01', 'run': 'r3', 'trial': 6},\n",
    "    {'sub': 'sub03', 'exp': 'exp01', 'run': 'r3', 'trial': 8},\n",
    "    {'sub': 'sub04', 'exp': 'exp01', 'run': 'r2', 'trial': 16},\n",
    "    {'sub': 'sub04', 'exp': 'exp01', 'run': 'r2', 'trial': 21},\n",
    "    {'sub': 'sub04', 'exp': 'exp01', 'run': 'r2', 'trial': 22},\n",
    "    {'sub': 'sub04', 'exp': 'exp01', 'run': 'r2', 'trial': 23},\n",
    "    {'sub': 'sub05', 'exp': 'exp01', 'run': 'r3', 'trial': 3},\n",
    "    {'sub': 'sub05', 'exp': 'exp01', 'run': 'r3', 'trial': 4},\n",
    "    {'sub': 'sub05', 'exp': 'exp01', 'run': 'r3', 'trial': 5},\n",
    "    {'sub': 'sub05', 'exp': 'exp01', 'run': 'r3', 'trial': 26},\n",
    "    {'sub': 'sub05', 'exp': 'exp01', 'run': 'r3', 'trial': 3},\n",
    "    {'sub': 'sub05', 'exp': 'exp01', 'run': 'r3', 'trial': 3},\n",
    "    {'sub': 'sub06', 'exp': 'exp01', 'run': 'r2', 'trial': 37},\n",
    "    {'sub': 'sub06', 'exp': 'exp02', 'run': 'r1', 'trial': 0},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r2', 'trial': 0},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r2', 'trial': 3},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r2', 'trial': 11},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r2', 'trial': 45},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r2', 'trial': 46},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r2', 'trial': 47},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r2', 'trial': 48},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r2', 'trial': 49},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r2', 'trial': 38},\n",
    "    {'sub': 'sub07', 'exp': 'exp01', 'run': 'r2', 'trial': 24},\n",
    "    {'sub': 'sub08', 'exp': 'exp02', 'run': 'r1', 'trial': 0},\n",
    "    {'sub': 'sub09', 'exp': 'exp01', 'run': 'r1', 'trial': 0},\n",
    "    {'sub': 'sub09', 'exp': 'exp01', 'run': 'r1', 'trial': 1},\n",
    "    {'sub': 'sub10', 'exp': 'exp01', 'run': 'r1', 'trial': 2},\n",
    "    {'sub': 'sub10', 'exp': 'exp01', 'run': 'r1', 'trial': 26},\n",
    "    {'sub': 'sub10', 'exp': 'exp01', 'run': 'r1', 'trial': 16},\n",
    "    {'sub': 'sub10', 'exp': 'exp01', 'run': 'r1', 'trial': 15},\n",
    "    {'sub': 'sub10', 'exp': 'exp01', 'run': 'r1', 'trial': 12},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r2', 'trial': 37},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r2', 'trial': 42},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 23},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 24},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 25},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 26},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 27},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 28},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 29},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 30},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 31},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 32},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 33},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 34},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 35},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 36},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 37},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 38},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 39},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 40},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 41},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 42},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 43},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 44},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 45},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 46},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 47},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 48},\n",
    "    {'sub': 'sub10', 'exp': 'exp02', 'run': 'r3', 'trial': 49},\n",
    "    {'sub': 'sub12', 'exp': 'exp01', 'run': 'r1', 'trial': 0},\n",
    "    {'sub': 'sub12', 'exp': 'exp02', 'run': 'r1', 'trial': 44},\n",
    "    {'sub': 'sub12', 'exp': 'exp02', 'run': 'r1', 'trial': 36},\n",
    "    {'sub': 'sub12', 'exp': 'exp02', 'run': 'r1', 'trial': 14},\n",
    "    {'sub': 'sub12', 'exp': 'exp01', 'run': 'r1', 'trial': 0},\n",
    "    {'sub': 'sub12', 'exp': 'exp02', 'run': 'r1', 'trial': 44},\n",
    "    {'sub': 'sub12', 'exp': 'exp02', 'run': 'r1', 'trial': 14},\n",
    "    {'sub': 'sub09', 'exp': 'exp01', 'run': 'r1', 'trial': 0},\n",
    "    {'sub': 'sub16', 'exp': 'exp01', 'run': 'r3', 'trial': 14},\n",
    "    {'sub': 'sub15', 'exp': 'exp01', 'run': 'r2', 'trial': 36},\n",
    "    {'sub': 'sub15', 'exp': 'exp01', 'run': 'r2', 'trial': 14},\n",
    "    {'sub': 'sub19', 'exp': 'exp01', 'run': 'r1', 'trial': 0}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic I/O and conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_eeg(path):\n",
    "    eeg = octave.pop_loadset(path)\n",
    "    new_trial_list = []\n",
    "    for i in range(eeg.data.shape[2]):\n",
    "        trial = eeg.data[:, :, i]\n",
    "        time = np.linspace(-1000, 1000, num=trial.shape[1])\n",
    "        trial = pd.DataFrame(np.transpose(trial), columns=eeg.chanlocs.labels[0])\n",
    "        trial['time'] = time\n",
    "        new_trial_list.append(trial)\n",
    "    return new_trial_list\n",
    "\n",
    "def read_trials_from_mat(filename):\n",
    "    x = loadmat(filename)\n",
    "    mat_trials = x['dat'][0][0][3][0]\n",
    "    trials = []\n",
    "    time = np.linspace(-1000, 1000, len(mat_trials[0][0]))\n",
    "    for mat_trial in mat_trials:\n",
    "        trials.append(pd.DataFrame({'C3': mat_trial[0], 'C4': mat_trial[1], 'time': time}))\n",
    "    return trials\n",
    "\n",
    "def crop_trials(trial_list, duration_millis=500, sampling_rate=2048):\n",
    "    new_trial_list = []\n",
    "    for trial in trial_list:\n",
    "        samples_to_pick = duration_millis * sampling_rate / 1000\n",
    "        new_trial_list.append(trial.tail(int(samples_to_pick)))\n",
    "    return new_trial_list, samples_to_pick\n",
    "\n",
    "def crop_mep_region(mep_frame, crop_start=0.211, crop_end=0.4):\n",
    "    multiplier = 7499 / 1.4998\n",
    "    start = int(crop_start * multiplier)\n",
    "    end = int(crop_end * multiplier)\n",
    "    mep_cropped = mep_frame.iloc[start:end, :]\n",
    "    time = mep_frame.iloc[start:end, :]\n",
    "    return mep_cropped\n",
    "\n",
    "def get_apb_column_name(mep_frame):\n",
    "    if 'L APB' in mep_frame:\n",
    "        return 'L APB'\n",
    "    else:\n",
    "        return 'APB'\n",
    "\n",
    "def open_mep_as_df(path):\n",
    "    fileMep = open(path, \"r+\")\n",
    "    mep_frames = fileMep.read().split('\\n\\n')\n",
    "    df_list = []\n",
    "    for mep_frame in mep_frames:\n",
    "        df_list.append(pd.read_csv(StringIO(mep_frame), '\\t'))\n",
    "    return df_list\n",
    "\n",
    "def get_mep_path(eeg_path):\n",
    "    segments = eeg_path.split('/')\n",
    "    segments[4] = 'mep'\n",
    "    segments = segments[:-1]\n",
    "    mep_path = '/'.join(segments) + '/*.txt'\n",
    "    mep_path = glob.glob(mep_path)[0]\n",
    "    return mep_path\n",
    "\n",
    "def calculate_mep_size(mep_frame):\n",
    "    mep_cropped = crop_mep_region(mep_frame)\n",
    "    apb_name = get_apb_column_name(mep_frame)\n",
    "    max_row = mep_frame.iloc[mep_cropped.idxmax(axis=0)[apb_name]]\n",
    "    min_row = mep_frame.iloc[mep_cropped.idxmin(axis=0)[apb_name]]\n",
    "    mep_size = max_row[apb_name] - min_row[apb_name]\n",
    "    return mep_size, min_row, max_row\n",
    "\n",
    "# Read CMAP file.\n",
    "def read_cmap(path):\n",
    "    df = pd.read_excel(path, sheet_name='Raw data', header=None)\n",
    "    columns = []\n",
    "    ix = 0\n",
    "    for r in df.index.values:\n",
    "        if df[0][r] == 'Channel':\n",
    "            ix = r\n",
    "            break\n",
    "    for c in df.columns.values:\n",
    "        if (df[c][ix] == 2):\n",
    "            columns.append(c)\n",
    "    for r in df.index.values:\n",
    "        if df[0][r] == 'Time (ms)':\n",
    "            ix = r\n",
    "            break\n",
    "    df2 = df[columns].iloc[np.linspace(ix+1, df.shape[0]-2, df.shape[0]-ix-2)]\n",
    "    df2['time'] = df[0].iloc[np.linspace(ix+1, df.shape[0]-2, df.shape[0]-ix-2)]\n",
    "    return df2\n",
    "\n",
    "# Identify CMAP peaks.\n",
    "def find_cmap_peaks(df):\n",
    "    df2 = df[(df['time'] > 105) * (df['time'] < 120)]\n",
    "    time = df2['time']\n",
    "    mean_clean = np.mean(df2.drop('time', axis=1), axis=1)\n",
    "    return mean_clean.max(), mean_clean.min()\n",
    "\n",
    "def get_cmap_path(eeg_path):\n",
    "    segments = eeg_path.split('/')\n",
    "    segments[4] = 'cmap'\n",
    "    del segments[5]\n",
    "    segments = segments[:-1]\n",
    "    cmap_path = '/'.join(segments) + '/*.xlsx'\n",
    "    cmap_paths = glob.glob(cmap_path)\n",
    "    if (len(cmap_paths) == 0):\n",
    "        print('No CMAP found for ' + cmap_path)\n",
    "        return ''\n",
    "    return cmap_paths[0]\n",
    "\n",
    "def get_mep_category_absolute_binary(mep_size):\n",
    "    if mep_size <= .5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def calculate_mep_categories_binary(mep_sizes):\n",
    "    p1 = np.percentile(mep_sizes, 50)\n",
    "    cat = np.ones(len(mep_sizes)) * (mep_sizes >= p1)\n",
    "    return cat\n",
    "\n",
    "def calculate_mep_categories_cmap_binary(mep_sizes, cmap):\n",
    "    sizes = []\n",
    "    for mep_size in mep_sizes:\n",
    "        sizes.append(mep_size / cmap)\n",
    "    p1 = np.percentile(sizes, 50)\n",
    "    cat = np.ones(len(mep_sizes)) * (sizes >= p1)\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEP latency and duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mep_latency_duration(mep_frame, plot=False, title=None):    \n",
    "    \n",
    "    # Calculate prestimulus mean amplitude and std.\n",
    "    std_limit = 5\n",
    "    peak_lookup_start_time = .215\n",
    "    apb_name = get_apb_column_name(mep_frame)\n",
    "    cropped = crop_mep_region(mep_frame, crop_start=0, crop_end=0.198)[apb_name]\n",
    "    mean_amp = np.mean(cropped.values)\n",
    "    std = np.std(cropped)\n",
    "    \n",
    "    # Find the index of the first point where amp > 5 SD of mean prestimulus\n",
    "    df_peak = mep_frame[mep_frame['s'] > peak_lookup_start_time]\n",
    "    peak_time = 0\n",
    "    for idx, row in df_peak.iterrows():\n",
    "        if (row[apb_name] > std_limit * std + mean_amp or row[apb_name] < mean_amp - std_limit * std):\n",
    "            peak_time = row['s']\n",
    "            break\n",
    "\n",
    "    latency_absolute = mep_frame[mep_frame['s'] == peak_time]['s'].values[0]\n",
    "    latency = (latency_absolute - 0.2)\n",
    "    \n",
    "    # Calculate the MEP duration.\n",
    "    std_limit = 6\n",
    "    sample_count = mep_frame[mep_frame['s'] < 0.03].shape[0]\n",
    "    df_mean = crop_mep_region(mep_frame, crop_start=0.26, crop_end=0.3)[apb_name]\n",
    "    mean_amp = np.mean(df_mean.values)\n",
    "    std = np.std(df_mean.values)\n",
    "    consecutive_baseline_count = 0\n",
    "    return_to_baseline_time = 0\n",
    "    cropped = crop_mep_region(mep_frame, crop_start=latency_absolute, crop_end=0.3)\n",
    "    for idx, row in cropped.iterrows():\n",
    "        if (row[apb_name] < mean_amp + std_limit * std) and (row[apb_name] > mean_amp - std_limit * std):\n",
    "            if consecutive_baseline_count == 0:\n",
    "                return_to_baseline_time = row['s']\n",
    "            consecutive_baseline_count += 1\n",
    "        else:\n",
    "            consecutive_baseline_count = 0\n",
    "        if consecutive_baseline_count > sample_count:\n",
    "            break\n",
    "    \n",
    "    if plot == True:\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.plot(mep_frame.iloc[0:2000]['s'], mep_frame.iloc[0:2000][apb_name])\n",
    "        plt.axvspan(latency_absolute, return_to_baseline_time, color='r', alpha=0.15)\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude (mV)')\n",
    "        if title != None:\n",
    "            plt.title(title)\n",
    "    return latency, return_to_baseline_time-0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power and phase calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackman_harris_filter(channel, time, cutoffs, fs):\n",
    "    numtaps = 801\n",
    "    b = signal.firwin(numtaps, cutoffs, window='blackmanharris', fs=fs)\n",
    "    filtered = signal.lfilter(b, 1, channel)\n",
    "    delay = 0.5 * (numtaps - 1) / fs\n",
    "    df = pd.DataFrame({\n",
    "        'time': time-delay,\n",
    "        'channel': filtered\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def get_phase(channel, time, band, fs=4096, plot=False):\n",
    "    df_filtered = blackman_harris_filter(channel, time, [0.000001, band[0]], fs)\n",
    "    df_filtered = blackman_harris_filter(df_filtered['channel'].values, df_filtered['time'].values, band[1], fs)    \n",
    "    hilb = signal.hilbert(df_filtered[(df_filtered['time'] > -750) * (df_filtered['time'] < -1)]['channel'])\n",
    "    phase = np.angle(hilb, deg=True)\n",
    "    df_phase = pd.DataFrame(phase, columns=['phase'])\n",
    "    df_phase['time'] = df_filtered[(df_filtered['time'] > -750) * (df_filtered['time'] < -1)]['time'].values\n",
    "    \n",
    "    if plot == True:\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(13,10))\n",
    "        ax1.plot(df_filtered['time'], df_filtered['channel'])\n",
    "        ax1.set_title('Filtered channel')\n",
    "        \n",
    "#         p = pburg(df_filtered['channel'], 1000, sampling=4096, NFFT=4096)\n",
    "#         power = p.psd\n",
    "#         freq = np.linspace(0, 4096, len(power))\n",
    "#         df_power = pd.DataFrame({'freq': freq, 'power': power})\n",
    "#         df_power = df_power[df_power['freq'] < 4096/2]\n",
    "#         ax2.plot(df_power['freq'], np.log(df_power['power']))\n",
    "#         ax2.set_xlim(5, 80)\n",
    "#         ax2.set_title('Power')\n",
    "        \n",
    "        ax3.plot(df_phase['time'], df_phase['phase'])\n",
    "        ax3.plot(df_phase.iloc[-1]['time'], df_phase.iloc[-1]['phase'], 'o')\n",
    "        ax3.set_title('Phase')\n",
    "        plt.show()\n",
    "    # df_phase = df_phase[df_phase['time'] <= -20]\n",
    "    return df_phase.iloc[-1]['phase'] + 180\n",
    "\n",
    "def get_power(channel, time, band, crop_start_millis=-150, fs=500, plot=False):       \n",
    "    # Crop.\n",
    "    channel = channel[(time > crop_start_millis) * (time < -1)].values\n",
    "    time = time[(time > crop_start_millis) * (time < -1)].values\n",
    "\n",
    "    # Resample.\n",
    "    ts = time[1] - time[0]\n",
    "    secs = len(channel) * ts\n",
    "    secs = secs/1000\n",
    "    resampled = signal.resample(channel, int(secs*fs))\n",
    "    resampled_time = np.linspace(time[0], time[-1], len(resampled))\n",
    "    \n",
    "    # Filer if gamma region.\n",
    "    if 48 > band[0] and 48 < band[1]:\n",
    "        df_filtered = blackman_harris_filter(resampled, resampled_time, [48, 52], fs)\n",
    "        resampled = df_filtered['channel']\n",
    "        resampled_time = df_filtered['time']\n",
    "    \n",
    "    # PSD.\n",
    "    try:\n",
    "        # Welch method\n",
    "#         freq, power = signal.welch(resampled, fs, nperseg=fs/2, nfft=2000)\n",
    "#         freq_res = freq[1] - freq[0]\n",
    "#         idx_band = np.logical_and(freq >= band[0], freq <= band[1])\n",
    "#         # mean_power = simps(power[idx_band], dx=freq_res)\n",
    "#         df_power = pd.DataFrame({'freq': freq, 'power': power})\n",
    "#         df_power_in_band = df_power[(df_power['freq'] >= band[0]) * (df_power['freq'] <= band[1])]\n",
    "#         mean_power = np.mean(df_power_in_band['power'])\n",
    "#         power = np.log(power)\n",
    "        \n",
    "#         # FFT method\n",
    "#         power = np.log(np.abs(np.fft.fft(resampled, n=500)))\n",
    "#         freq = np.linspace(0, fs, len(power))\n",
    "        \n",
    "#         # Burgs method\n",
    "        p = pburg(resampled, int(len(resampled)/2), sampling=fs, NFFT=4096)\n",
    "        power = p.psd\n",
    "        freq = np.linspace(0, fs, len(power))\n",
    "        df_power = pd.DataFrame({'freq': freq, 'power': power})\n",
    "        df_power = df_power[df_power['freq'] < fs/2]\n",
    "        df_power_in_band = df_power[(df_power['freq'] >= band[0]) * (df_power['freq'] <= band[1])]\n",
    "        mean_power = np.mean(df_power_in_band['power'])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    if plot == True:\n",
    "        plt.figure()\n",
    "        plt.plot(df_power['freq'], np.log(df_power['power']))\n",
    "        plt.axvspan(band[0], band[1], color='r', alpha=0.2)\n",
    "        plt.axhline(mean_power, color='r')\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Log Power')\n",
    "        plt.xlim([5, 100])\n",
    "\n",
    "    return mean_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert set files to pickle files.\n",
    "# for eeg_path in tqdm_notebook(eegs):\n",
    "#     path = os.path.dirname(eeg_path)\n",
    "#     print(path + '/06-clean-prestimulus.set')\n",
    "#     files = glob.glob(path + '/06-clean-prestimulus.set')\n",
    "#     if len(files) == 0:\n",
    "#         continue\n",
    "#     trials = read_eeg(files[0])\n",
    "#     pickle.dump(trials, open(path + \"/06-clean-prestimulus.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials = read_trials_from_mat(eegs[39])\n",
    "# trial = trials[15]\n",
    "# trial = trial[trial['time'] < -5]\n",
    "# plt.plot(trial['time'], trial['C3'])\n",
    "# plt.show()\n",
    "# get_power(trial['C3'], trial['time'], [8, 12], plot=True)\n",
    "# get_phase(trial['C3'], trial['time'], [13, 30], plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine EEG and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power_phase(trial):    \n",
    "    phase_power_dict = {}\n",
    "    for channel in ['C3', 'C4']:        \n",
    "        phase_power_dict[channel + '_theta_phase'] = get_phase(trial[channel], trial['time'], [3.5, 8])\n",
    "        phase_power_dict[channel + '_mu_phase'] = get_phase(trial[channel], trial['time'], [8, 12])\n",
    "        phase_power_dict[channel + '_beta_phase'] = get_phase(trial[channel], trial['time'], [13, 30])\n",
    "        phase_power_dict[channel + '_gamma_phase'] = get_phase(trial[channel], trial['time'], [30, 80])\n",
    "        phase_power_dict[channel + '_low_beta_phase'] = get_phase(trial[channel], trial['time'], [12, 20])\n",
    "        phase_power_dict[channel + '_high_beta_phase'] = get_phase(trial[channel], trial['time'], [20, 30])\n",
    "        phase_power_dict[channel + '_low_gamma_phase'] = get_phase(trial[channel], trial['time'], [30, 46])\n",
    "        phase_power_dict[channel + '_high_gamma_phase'] = get_phase(trial[channel], trial['time'], [46, 70])\n",
    "        \n",
    "        phase_power_dict[channel + '_theta_power'] = get_power(trial[channel], trial['time'], [3.5, 8], crop_start_millis=-1000)\n",
    "        phase_power_dict[channel + '_mu_power'] = get_power(trial[channel], trial['time'], [8, 12])\n",
    "        phase_power_dict[channel + '_beta_power'] = get_power(trial[channel], trial['time'], [13, 30])\n",
    "        phase_power_dict[channel + '_gamma_power'] = get_power(trial[channel], trial['time'], [30, 80])\n",
    "        phase_power_dict[channel + '_low_beta_power'] = get_power(trial[channel], trial['time'], [12, 20])\n",
    "        phase_power_dict[channel + '_high_beta_power'] = get_power(trial[channel], trial['time'], [20, 30])\n",
    "        phase_power_dict[channel + '_low_gamma_power'] = get_power(trial[channel], trial['time'], [30, 46])\n",
    "        phase_power_dict[channel + '_high_gamma_power'] = get_power(trial[channel], trial['time'], [46, 70])\n",
    "    return phase_power_dict\n",
    "        \n",
    "def process_trials(eeg_path):\n",
    "    sub = eeg_path.split('/')[2]\n",
    "    exp = eeg_path.split('/')[3]\n",
    "    run = eeg_path.split('/')[5]\n",
    "    \n",
    "    trials = pickle.load(open(os.path.dirname(eeg_path) + \"/06-clean-prestimulus.p\", \"rb\"))\n",
    "#     files = glob.glob(os.path.dirname(eeg_path) + \"/06-clean-prestimulus-hjorth.mat\")\n",
    "#     if (len(files) == 0):\n",
    "#         return []\n",
    "#     trials = read_trials_from_mat(os.path.dirname(eeg_path) + \"/06-clean-prestimulus-hjorth.mat\")\n",
    "\n",
    "    # Calculate CMAP.\n",
    "    try:\n",
    "        cmap_max, cmap_min = find_cmap_peaks(read_cmap(get_cmap_path(eeg_path)))\n",
    "        cmap = cmap_max - cmap_min\n",
    "    except:\n",
    "        cmap = 0\n",
    "\n",
    "    # Calculate MEP categories.\n",
    "    try:\n",
    "        mep_list = open_mep_as_df(get_mep_path(eeg_path))\n",
    "        mep_sizes = []\n",
    "        for mep_df in mep_list:\n",
    "            size, _, _ = calculate_mep_size(mep_df)\n",
    "            mep_sizes.append(size)\n",
    "        mep_cmap_list = calculate_mep_categories_cmap_binary(mep_sizes, cmap)\n",
    "        mep_percentile_list = calculate_mep_categories_binary(mep_sizes)\n",
    "    except IndexError as e:\n",
    "        print('Skipping ' + sub + '/' + exp + '/' + run + '/' + str(trial_num) + ': MEP file error')\n",
    "        return []\n",
    "    \n",
    "    # Calculate powers and phases. And combine the results in a df list.\n",
    "    power_phase_list = []\n",
    "    for trial_num in range(len(trials)):\n",
    "        trial = trials[trial_num]\n",
    "    \n",
    "        # Ignore rejected trials.\n",
    "        break_loop = False\n",
    "        for rejected in rejected_trials:\n",
    "            if (sub == rejected['sub']) and (exp == rejected['exp']) and (rejected['run'] == run.split(' ')[2]) and ((rejected['trial'] == trial_num) or (rejected['trial'] == '*')):\n",
    "                print('Rejecting ' + sub + '/' + exp + '/' + run + '/' + str(rejected['trial']))\n",
    "                break_loop = True\n",
    "                break\n",
    "        if break_loop == True:\n",
    "            break\n",
    "            \n",
    "        # Calculate MEP latency and duration.\n",
    "        latency, duration = calculate_mep_latency_duration(mep_list[trial_num])\n",
    "        if duration == 0:\n",
    "            print('Skipping ' + sub + '/' + exp + '/' + run + '/' + str(trial_num) + ': because duration is 0')\n",
    "            continue\n",
    "        \n",
    "        # Calculate phase and power.\n",
    "        print('Processing ' + sub + '/' + exp + '/' + run + '/' + str(trial_num))\n",
    "        try:\n",
    "            power = calculate_power_phase(trial)\n",
    "        except ValueError as e:\n",
    "            print('Skipping ' + sub + '/' + exp + '/' + run + '/' + str(trial_num) + ': ' + str(e))\n",
    "            continue\n",
    "\n",
    "        power_phase_dict = power\n",
    "        power_phase_dict['sub'] = sub\n",
    "        power_phase_dict['exp'] = exp\n",
    "        power_phase_dict['run'] = run\n",
    "        power_phase_dict['trial_num'] = trial_num\n",
    "        power_phase_dict['cmap'] = cmap\n",
    "        power_phase_dict['mep_size'] = mep_sizes[trial_num]\n",
    "        power_phase_dict['mep_cat_abs'] = get_mep_category_absolute_binary(mep_sizes[trial_num])\n",
    "        power_phase_dict['mep_cat_percentile'] = mep_percentile_list[trial_num]\n",
    "        power_phase_dict['mep_cat_cmap'] = mep_cmap_list[trial_num]\n",
    "        power_phase_dict['mep_by_cmap'] = mep_sizes[trial_num] / cmap\n",
    "        power_phase_dict['mep_latency'] = latency\n",
    "        power_phase_dict['mep_duration'] = duration\n",
    "        power_phase_list.append(power_phase_dict)\n",
    "    return power_phase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()-2\n",
    "results = Parallel(n_jobs=num_cores)(delayed(process_trials)(eeg_path) for eeg_path in tqdm_notebook(eegs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_powers = []\n",
    "for result in results:\n",
    "    if len(result) == 0:\n",
    "        continue\n",
    "    for trial in result:\n",
    "        df_powers.append(trial)\n",
    "len(df_powers)\n",
    "\n",
    "df_power = pd.DataFrame(df_powers)\n",
    "print(df_power.shape)\n",
    "df_power.to_excel('136-clean-v1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove outliers based on STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power2 = df_power.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power2 = df_power[df_power['mep_latency'] < 0.06]\n",
    "# df_power2 = df_power2[df_power2['mep_latency'] > -0.1]\n",
    "# df_power2 = df_power2[np.log(df_power2['C3_theta_power']) > -4]\n",
    "# df_power2 = df_power2[np.log(df_power2['C3_mu_power']) > -6]\n",
    "# df_power2 = df_power2[np.log(df_power2['C3_beta_power']) > -5.5]\n",
    "# df_power2 = df_power2[np.log(df_power2['C3_gamma_power']) > -6]\n",
    "\n",
    "# df_power2 = df_power2[np.log(df_power2['C3_theta_power']) > 0]\n",
    "# df_power2 = df_power2[np.log(df_power2['C3_mu_power']) > -1]\n",
    "# df_power2 = df_power2[np.log(df_power2['C3_beta_power']) > -1]\n",
    "# df_power2 = df_power2[np.log(df_power2['C3_gamma_power']) > -1.8]\n",
    "\n",
    "print(df_power2.shape)\n",
    "df_power2.to_excel('136-clean-v1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.set(font='serif')\n",
    "sns.set_style(\"white\", {\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\", \"Palatino\", \"serif\"]\n",
    "})\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "\n",
    "plot_y = 'mep_duration'\n",
    "\n",
    "sns.catplot(x=\"sub\", y=plot_y, kind=\"box\", data=df_power2, height=5, aspect=2)\n",
    "\n",
    "# mean = np.mean(df_power[plot_y].values)\n",
    "# std = np.std(df_power[plot_y].values)\n",
    "# plt.axhline(mean)\n",
    "# plt.axhspan(mean-2*std, mean+2*std, color='r', alpha=0.1)\n",
    "\n",
    "plt.xlabel('Subjects')\n",
    "plt.ylabel(plot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power2['C3_theta_phase'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power2['mep_size_log'] = np.log(df_power2['mep_size'])\n",
    "df_power2['C3_theta_power_log'] = np.log(df_power2['C3_theta_power'])\n",
    "df_power2['C3_mu_power_log'] = np.log(df_power2['C3_mu_power'])\n",
    "df_power2['C3_beta_power_log'] = np.log(df_power2['C3_beta_power'])\n",
    "df_power2['C3_gamma_power_log'] = np.log(df_power2['C3_gamma_power'])\n",
    "\n",
    "def get_phase_bin(df_data, band):\n",
    "    df_data['C3_'+band+'_phase_bin'] = 0\n",
    "    df_data.loc[(df_data['C3_'+band+'_phase'] >= 45) & (df_data['C3_'+band+'_phase'] <= 135), 'C3_'+band+'_phase_bin'] = 1\n",
    "    df_data.loc[(df_data['C3_'+band+'_phase'] >= 225) & (df_data['C3_'+band+'_phase'] <= 315), 'C3_'+band+'_phase_bin'] = 2\n",
    "    return df_data\n",
    "df_power2 = get_phase_bin(df_power2, 'theta')\n",
    "df_power2 = get_phase_bin(df_power2, 'mu')\n",
    "df_power2 = get_phase_bin(df_power2, 'beta')\n",
    "df_power2 = get_phase_bin(df_power2, 'gamma')\n",
    "\n",
    "# df_power2.to_excel('134-hjorth-v1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relations(x_label):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(12,4))\n",
    "    sns.regplot(x=x_label, y=\"mep_size_log\", data=df_power2, ax=ax1)\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel('MEP Size')\n",
    "\n",
    "    sns.regplot(x=x_label, y=\"mep_latency\", data=df_power2, ax=ax2)\n",
    "    ax2.set_xlabel(x_label)\n",
    "    ax2.set_ylabel('MEP Latency')\n",
    "    ax2.set_ylim(0.012, 0.04)\n",
    "\n",
    "    sns.regplot(x=x_label, y=\"mep_duration\", data=df_power2, ax=ax3)\n",
    "    ax3.set_xlabel(x_label)\n",
    "    ax3.set_ylabel('MEP Duration')\n",
    "    ax3.set_ylim(0.025, 0.065)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_relations('C3_theta_power_log')\n",
    "plot_relations('C3_mu_power_log')\n",
    "plot_relations('C3_beta_power_log')\n",
    "plot_relations('C3_gamma_power_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "sns.regplot(x=\"mep_size_log\", y=\"mep_duration\", data=df_power2, ax=ax1)\n",
    "ax1.set_xlabel('MEP Size')\n",
    "ax1.set_ylabel('MEP Duration')\n",
    "\n",
    "sns.regplot(x=\"mep_size\", y=\"mep_latency\", data=df_power2, ax=ax2)\n",
    "ax2.set_xlabel('MEP Size')\n",
    "ax2.set_ylabel('MEP Latency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power2[df_power2['mep_size_log'] < -3][['sub', 'exp', 'run', 'trial_num', 'mep_duration', 'mep_size', 'mep_latency', 'C3_beta_power_log']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power2.sort_values(by='C3_beta_power_log', ascending=True)[['sub', 'exp', 'run', 'trial_num', 'mep_duration', 'mep_size', 'mep_latency', 'C3_beta_power_log']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power.sort_values(by='mep_duration', ascending='True')[['sub', 'exp', 'run', 'trial_num', 'mep_duration', 'mep_size', 'mep_latency']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df_data, channel = 'C3'):\n",
    "    mu_power_mean = np.mean(df_data[channel + '_mu_power'])\n",
    "    mu_power_std = np.std(df_data[channel + '_mu_power'])\n",
    "    beta_power_mean = np.mean(df_data[channel + '_beta_power'])\n",
    "    beta_power_std = np.std(df_data[channel + '_beta_power'])\n",
    "    gamma_power_mean = np.mean(df_data[channel + '_gamma_power'])\n",
    "    gamma_power_std = np.std(df_data[channel + '_gamma_power'])\n",
    "    theta_power_mean = np.mean(df_data[channel + '_theta_power'])\n",
    "    theta_power_std = np.std(df_data[channel + '_theta_power'])\n",
    "\n",
    "    mep_size_mean = np.mean(df_data['mep_size'])\n",
    "    mep_size_std = np.std(df_data['mep_size'])\n",
    "    mep_duration_mean = np.mean(df_data['mep_duration'])\n",
    "    mep_duration_std = np.std(df_data['mep_duration'])\n",
    "    mep_latency_mean = np.mean(df_data['mep_latency'])\n",
    "    mep_latency_std = np.std(df_data['mep_latency'])\n",
    "\n",
    "    df_powers2 = []\n",
    "    for idx, row in tqdm_notebook(df_data.iterrows(), total=df_data.shape[0]):\n",
    "        if (\n",
    "#             row[channel + '_mu_power'] > mu_power_mean + 2 * mu_power_std or row[channel + '_mu_power'] < mu_power_mean - 2 * mu_power_std or \n",
    "#             row[channel + '_beta_power'] > beta_power_mean + 2 * beta_power_std or row[channel + '_beta_power'] < beta_power_mean - 2 * beta_power_std or \n",
    "#             row[channel + '_gamma_power'] > gamma_power_mean + 2 * gamma_power_std or row[channel + '_gamma_power'] < gamma_power_mean - 2 * gamma_power_std or \n",
    "#             row[channel + '_theta_power'] > theta_power_mean + 2 * theta_power_std or row[channel + '_theta_power'] < theta_power_mean - 2 * theta_power_std or \n",
    "#             row['mep_size'] > mep_size_mean + 2 * mep_size_std or row['mep_size'] < mep_size_mean - 2 * mep_size_std or\n",
    "            row['mep_duration'] > mep_duration_mean + 2 * mep_duration_std or row['mep_duration'] < mep_duration_mean - 2 * mep_duration_std or\n",
    "            row['mep_latency'] > mep_latency_mean + 2 * mep_latency_std or row['mep_latency'] < mep_latency_mean - 2 * mep_latency_std):\n",
    "            i = 1\n",
    "        else:\n",
    "            df_powers2.append(row)                \n",
    "    df_powers2 = pd.DataFrame(df_powers2)\n",
    "    return df_powers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_powers2 = remove_outliers(df_power)\n",
    "print(df_powers2.shape)\n",
    "df_powers2.to_excel('134-ft-hjorth-no-outlier-v1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_powers2['sub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_sub(df_data):\n",
    "    df_data2 = []\n",
    "    for sub in df_data['sub'].unique():\n",
    "        df_new = df_data[df_data['sub'] == sub]\n",
    "        mep_size_mean = np.mean(df_new['mep_size'])\n",
    "        mep_size_std = np.std(df_new['mep_size'])\n",
    "        for idx, row in df_new.iterrows():\n",
    "            if row['mep_size'] < mep_size_mean + 2 * mep_size_std and row['mep_size'] > mep_size_mean - 2 * mep_size_std:\n",
    "                df_data2.append(row)\n",
    "    df_data2 = pd.DataFrame(df_data2)\n",
    "    return df_data2\n",
    "\n",
    "df_powers3 = remove_outliers_sub(df_powers2)\n",
    "print(df_powers3.shape)\n",
    "df_powers3.to_excel('134-ft-hjorth-no-outlier-sub-v1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_powers2['mep_latency'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(df_powers2['mep_duration']).hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
